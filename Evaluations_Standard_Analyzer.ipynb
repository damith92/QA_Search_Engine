{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53a0d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import lucene\n",
    "\n",
    "from java.io import File\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, StringField, FieldType\n",
    "from org.apache.lucene.search import IndexSearcher, BooleanClause\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions, IndexReader, DirectoryReader\n",
    "from org.apache.lucene.queryparser.classic import MultiFieldQueryParser, QueryParserBase\n",
    "from org.apache.lucene.store import SimpleFSDirectory, FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.search.similarities import ClassicSimilarity, BM25Similarity, LMDirichletSimilarity, BooleanSimilarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3f775d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading ground truth table\n",
    "pd_grdtrth = pd.read_csv('data/cleaned_grnd_truth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbb0290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_trth_list = pd_grdtrth[['qid1','qid2','label']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "672f831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_tup_list = [(each[0], each[1], each[2]) for each in grd_trth_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cdf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10c948dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexer function\n",
    "def indexing_docs(file_path,similarity,index_pth_str=\"index/\"):\n",
    "    lucene.initVM()\n",
    "    indexPath = File(index_pth_str).toPath()\n",
    "    indexDir = FSDirectory.open(indexPath)\n",
    "    writerConfig = IndexWriterConfig(StandardAnalyzer())\n",
    "    writerConfig.setSimilarity(similarity)\n",
    "    writer = IndexWriter(indexDir, writerConfig)\n",
    "   \n",
    "    t1 = FieldType()\n",
    "    t1.setStored(True)\n",
    "    t1.setTokenized(False)\n",
    "    t1.setIndexOptions(IndexOptions.NONE)\n",
    "   \n",
    "    t2 = FieldType()\n",
    "    t2.setStored(False)\n",
    "    t2.setTokenized(True)\n",
    "    t2.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "   \n",
    "    df_file = pd.read_csv(file_path)\n",
    "    cntr = 0\n",
    "    for i, row in df_file.iterrows():\n",
    "        doc = Document()\n",
    "        doc.add(Field(\"qid\", row['qid'], t1))\n",
    "        doc.add(Field(\"qn_title\", row['qn_title'], t1))\n",
    "        doc.add(Field(\"qn_link\", row['qn_link'], t1))\n",
    "        doc.add(Field(\"qns_title_processed\", row['qns_title_processed'], t2))\n",
    "        doc.add(Field(\"qns_body_processed\", row['qns_body_processed'], t2))\n",
    "        doc.add(Field(\"ans_body_processed\", row['ans_body_processed'], t2))\n",
    "        writer.addDocument(doc)\n",
    "        cntr += 1\n",
    "    \n",
    "    print(\"Indexing Successful\")   \n",
    "    writer.close()\n",
    "    indexDir.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea69c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d308c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreiver function\n",
    "def retriever_fn(search_string, similarity, index_pth_str=\"index/\"):\n",
    "    \n",
    "    analyzer = StandardAnalyzer()\n",
    "    indexPath = File(index_pth_str).toPath()\n",
    "    indexDir = FSDirectory.open(indexPath)\n",
    "    reader = DirectoryReader.open(indexDir)\n",
    "    searcher = IndexSearcher(reader)\n",
    "    searcher.setSimilarity(similarity)\n",
    "\n",
    "    fields = ['qns_title_processed', 'qns_body_processed', 'ans_body_processed']\n",
    "    flags = [BooleanClause.Occur.SHOULD, BooleanClause.Occur.SHOULD, BooleanClause.Occur.SHOULD]\n",
    "    query_parser = MultiFieldQueryParser(fields, analyzer)\n",
    "    query = query_parser.parse(search_string, fields, flags, analyzer)\n",
    "    MAX = 50\n",
    "    start_time = time.time()\n",
    "    hits = searcher.search(query, MAX)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    indexDir.close()\n",
    "    \n",
    "    results_list = []\n",
    "    for ind,hit in enumerate(hits.scoreDocs):\n",
    "        doc = searcher.doc(hit.doc)\n",
    "        results_list.append((int(doc.get(\"qid\")), float(hit.score)))\n",
    "        \n",
    "    return results_list, retrieval_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284fab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a51a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_ranking, query_id, qrels, k=5):\n",
    "\n",
    "  \n",
    "    retrieved = [doc[0] for doc in doc_ranking[:k]] # take only the document id, rather than score\n",
    "\n",
    "    qrels_query = [qrel for qrel in qrels if qrel[0] == query_id] # iterate through the relevance judgements and return rows which are relevant to given query\n",
    "    relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 1] # retrieve the ids of documents that have positive relevance judgements (i.e relevant documents)\n",
    "    non_relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 0] # retrieve the ids of documents that have 0 relevance judgements (i.e non relevant documents)\n",
    "\n",
    "    TP = len(set(retrieved) & set(relevant_doc_ids)) # intersection between retrieved documents and relevant documents. num of docs in intersection = TP (positive examples that are correctly identified)\n",
    "    FP = len(set(retrieved) & set(non_relevant_doc_ids)) # intersection between retrieved documents and non relevant documents. num of docs in interesetion is FP (negative examples that are incorrectly identifed are positive)\n",
    "\n",
    "    if TP+FP >0:\n",
    "        precision = TP / (TP + FP)\n",
    "       \n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "\n",
    "    return TP, FP, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf87044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecc715a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k(doc_ranking, query_id, qrels, k=5):\n",
    "  \n",
    "    retrieved = [doc[0] for doc in doc_ranking[:k]] # take only the document id, rather than score\n",
    "  \n",
    "    qrels_query = [qrel for qrel in qrels if qrel[0] == query_id] # iterate through the relevance judgements and return rows which are relevant to given query\n",
    "    relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 1] # retrieve the ids of documents that have positive relevance judgements (i.e relevant documents)\n",
    "    non_relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 0] # retrieve the ids of documents that have 0 relevance judgements (i.e non relevant documents)\n",
    "\n",
    "    TP = len(set(retrieved) & set(relevant_doc_ids)) # intersection between retrieved documents and relevant documents. num of docs in intersection = TP (positive examples that are correctly identified)\n",
    "    FP = len(set(retrieved) & set(non_relevant_doc_ids)) # intersection between retrieved documents and non relevant documents. num of docs in interesetion is FP (negative examples that are incorrectly identifed are positive)\n",
    "    FN = len(set(relevant_doc_ids) - set(retrieved)) # relevance docs minus the retrieved docs equal FN (positive examples that are incorrectly identified as negative)\n",
    "\n",
    "    if TP+FP >0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "    if TP+FN >0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "        \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)  \n",
    "    else:\n",
    "        f1 = 0\n",
    "  \n",
    "    return f1, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff90dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43030e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ebcda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = pd_test[['qid','query']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545092a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cafc8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 1\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = ClassicSimilarity()\n",
    "index_pth = \"index7/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9aa8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.5307017543859649\n",
      "Average Recall at 5 = 0.6052631578947368\n",
      "Average F-1 Score at 5 = 0.5451127819548872\n",
      "Average Retrieval Time =  0.0011686776813707854\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eee12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5175438596491228\n",
      "Average Recall at 10 = 0.6052631578947368\n",
      "Average F-1 Score at 10 = 0.5350877192982457\n",
      "Average Retrieval Time =  0.0010557802099930612\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf5330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b1c408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 2\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BooleanSimilarity()\n",
    "index_pth = \"index8/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3125f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.5263157894736842\n",
      "Average Recall at 5 = 0.48245614035087714\n",
      "Average F-1 Score at 5 = 0.4982456140350877\n",
      "Average Retrieval Time =  0.0004878671545731394\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0963f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5131578947368421\n",
      "Average Recall at 10 = 0.5\n",
      "Average F-1 Score at 10 = 0.5012531328320802\n",
      "Average Retrieval Time =  0.0005195642772473787\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2616899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d21ab493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 3\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = LMDirichletSimilarity()\n",
    "index_pth = \"index9/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8eef3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.3815789473684211\n",
      "Average Recall at 5 = 0.42105263157894735\n",
      "Average F-1 Score at 5 = 0.39598997493734334\n",
      "Average Retrieval Time =  0.0010156882436651933\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f57ce394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.42105263157894735\n",
      "Average Recall at 10 = 0.4473684210526316\n",
      "Average F-1 Score at 10 = 0.42105263157894735\n",
      "Average Retrieval Time =  0.0018329620361328125\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172e1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54858177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 4\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BM25Similarity()\n",
    "index_pth = \"index10/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bde1696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.4868421052631579\n",
      "Average Recall at 5 = 0.5526315789473685\n",
      "Average F-1 Score at 5 = 0.5012531328320802\n",
      "Average Retrieval Time =  0.0018689381448846114\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6d12681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5175438596491228\n",
      "Average Recall at 10 = 0.6052631578947368\n",
      "Average F-1 Score at 10 = 0.5350877192982457\n",
      "Average Retrieval Time =  0.0007659008628443667\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c143a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4976c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 5\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BM25Similarity(1.0,0.5)\n",
    "index_pth = \"index11/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59a8777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.5131578947368421\n",
      "Average Recall at 5 = 0.5526315789473685\n",
      "Average F-1 Score at 5 = 0.518796992481203\n",
      "Average Retrieval Time =  0.00046836702447188526\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c712471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5175438596491228\n",
      "Average Recall at 10 = 0.6052631578947368\n",
      "Average F-1 Score at 10 = 0.5350877192982457\n",
      "Average Retrieval Time =  0.0009924738030684622\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615735a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d322b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 6\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BM25Similarity(6.0,0.5)\n",
    "index_pth = \"index12/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "757dd148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.4605263157894737\n",
      "Average Recall at 5 = 0.5\n",
      "Average F-1 Score at 5 = 0.4661654135338346\n",
      "Average Retrieval Time =  0.0008442276402523643\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd08fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.46491228070175433\n",
      "Average Recall at 10 = 0.5526315789473685\n",
      "Average F-1 Score at 10 = 0.48245614035087714\n",
      "Average Retrieval Time =  0.0006001497569837069\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94122e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d7f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab39ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c4931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c876d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
