{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a0d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import lucene\n",
    "\n",
    "from java.io import File\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.document import Document, Field, StringField, FieldType\n",
    "from org.apache.lucene.search import IndexSearcher, BooleanClause\n",
    "from org.apache.lucene.index import IndexWriter, IndexWriterConfig, IndexOptions, IndexReader, DirectoryReader\n",
    "from org.apache.lucene.queryparser.classic import MultiFieldQueryParser, QueryParserBase\n",
    "from org.apache.lucene.store import SimpleFSDirectory, FSDirectory\n",
    "from org.apache.lucene.util import Version\n",
    "from org.apache.lucene.search.similarities import ClassicSimilarity, BM25Similarity, LMDirichletSimilarity, BooleanSimilarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11a8bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>qn_title</th>\n",
       "      <th>qn_link</th>\n",
       "      <th>qns_title_processed</th>\n",
       "      <th>qns_body_processed</th>\n",
       "      <th>ans_body_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5537876</td>\n",
       "      <td>Get UTC offset from time zone name in python</td>\n",
       "      <td>https://stackoverflow.com/questions/5537876/ge...</td>\n",
       "      <td>get utc offset time zone name python</td>\n",
       "      <td>get utc offset time zone name python example a...</td>\n",
       "      <td>dst daylight saving time result depends time y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22741030</td>\n",
       "      <td>How to input matrix (2D list) in Python?</td>\n",
       "      <td>https://stackoverflow.com/questions/22741030/h...</td>\n",
       "      <td>input matrix d list python</td>\n",
       "      <td>new python usually use matlab gnu octave matri...</td>\n",
       "      <td>problem initialization step code actually make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31818050</td>\n",
       "      <td>Round number to nearest integer</td>\n",
       "      <td>https://stackoverflow.com/questions/31818050/r...</td>\n",
       "      <td>python round number nearest integer</td>\n",
       "      <td>trying round long float numbers like success f...</td>\n",
       "      <td>round change integer edit assigning int round ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2850893</td>\n",
       "      <td>Reading binary data from stdin</td>\n",
       "      <td>https://stackoverflow.com/questions/2850893/re...</td>\n",
       "      <td>reading binary data stdin</td>\n",
       "      <td>possible read stdin binary data python see fai...</td>\n",
       "      <td>docs see standard streams text mode default wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19726663</td>\n",
       "      <td>How to save the Pandas dataframe/series data a...</td>\n",
       "      <td>https://stackoverflow.com/questions/19726663/h...</td>\n",
       "      <td>save pandas dataframe series data figure</td>\n",
       "      <td>sounds somewhat weird need save pandas console...</td>\n",
       "      <td>use matplotlib table functionality additional ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4260116</td>\n",
       "      <td>Find size and free space of the filesystem con...</td>\n",
       "      <td>https://stackoverflow.com/questions/4260116/fi...</td>\n",
       "      <td>find size free space filesystem containing giv...</td>\n",
       "      <td>using python linux fastest way determine parti...</td>\n",
       "      <td>give name partition get filesystem statistics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27820178</td>\n",
       "      <td>How to add placeholder to an Entry in tkinter?</td>\n",
       "      <td>https://stackoverflow.com/questions/27820178/h...</td>\n",
       "      <td>add placeholder entry tkinter</td>\n",
       "      <td>created login window tkinter two entry field f...</td>\n",
       "      <td>need set default value entry like want delete ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13631908</td>\n",
       "      <td>Flask sessions not persisting on heroku</td>\n",
       "      <td>https://stackoverflow.com/questions/13631908/f...</td>\n",
       "      <td>flask sessions persisting heroku</td>\n",
       "      <td>user logs sample application add username key ...</td>\n",
       "      <td>well seeing indeed would answer due multiple w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32932230</td>\n",
       "      <td>Add an image in a specific position in the doc...</td>\n",
       "      <td>https://stackoverflow.com/questions/32932230/a...</td>\n",
       "      <td>add image specific position document docx python</td>\n",
       "      <td>use python docx generate microsoft word docume...</td>\n",
       "      <td>quoting document add picture method adds speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>337</td>\n",
       "      <td>XML Processing in Python</td>\n",
       "      <td>https://stackoverflow.com/questions/337/xml-pr...</td>\n",
       "      <td>xml processing python</td>\n",
       "      <td>build piece project need construct post xml do...</td>\n",
       "      <td>personally played several built options xml he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        qid                                           qn_title  \\\n",
       "0   5537876       Get UTC offset from time zone name in python   \n",
       "1  22741030           How to input matrix (2D list) in Python?   \n",
       "2  31818050                    Round number to nearest integer   \n",
       "3   2850893                     Reading binary data from stdin   \n",
       "4  19726663  How to save the Pandas dataframe/series data a...   \n",
       "5   4260116  Find size and free space of the filesystem con...   \n",
       "6  27820178     How to add placeholder to an Entry in tkinter?   \n",
       "7  13631908            Flask sessions not persisting on heroku   \n",
       "8  32932230  Add an image in a specific position in the doc...   \n",
       "9       337                           XML Processing in Python   \n",
       "\n",
       "                                             qn_link  \\\n",
       "0  https://stackoverflow.com/questions/5537876/ge...   \n",
       "1  https://stackoverflow.com/questions/22741030/h...   \n",
       "2  https://stackoverflow.com/questions/31818050/r...   \n",
       "3  https://stackoverflow.com/questions/2850893/re...   \n",
       "4  https://stackoverflow.com/questions/19726663/h...   \n",
       "5  https://stackoverflow.com/questions/4260116/fi...   \n",
       "6  https://stackoverflow.com/questions/27820178/h...   \n",
       "7  https://stackoverflow.com/questions/13631908/f...   \n",
       "8  https://stackoverflow.com/questions/32932230/a...   \n",
       "9  https://stackoverflow.com/questions/337/xml-pr...   \n",
       "\n",
       "                                 qns_title_processed  \\\n",
       "0              get utc offset time zone name python    \n",
       "1                        input matrix d list python    \n",
       "2               python round number nearest integer    \n",
       "3                         reading binary data stdin    \n",
       "4          save pandas dataframe series data figure    \n",
       "5  find size free space filesystem containing giv...   \n",
       "6                     add placeholder entry tkinter    \n",
       "7                  flask sessions persisting heroku    \n",
       "8  add image specific position document docx python    \n",
       "9                             xml processing python    \n",
       "\n",
       "                                  qns_body_processed  \\\n",
       "0  get utc offset time zone name python example a...   \n",
       "1  new python usually use matlab gnu octave matri...   \n",
       "2  trying round long float numbers like success f...   \n",
       "3  possible read stdin binary data python see fai...   \n",
       "4  sounds somewhat weird need save pandas console...   \n",
       "5  using python linux fastest way determine parti...   \n",
       "6  created login window tkinter two entry field f...   \n",
       "7  user logs sample application add username key ...   \n",
       "8  use python docx generate microsoft word docume...   \n",
       "9  build piece project need construct post xml do...   \n",
       "\n",
       "                                  ans_body_processed  \n",
       "0  dst daylight saving time result depends time y...  \n",
       "1  problem initialization step code actually make...  \n",
       "2  round change integer edit assigning int round ...  \n",
       "3  docs see standard streams text mode default wr...  \n",
       "4  use matplotlib table functionality additional ...  \n",
       "5  give name partition get filesystem statistics ...  \n",
       "6  need set default value entry like want delete ...  \n",
       "7  well seeing indeed would answer due multiple w...  \n",
       "8  quoting document add picture method adds speci...  \n",
       "9  personally played several built options xml he...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/cleaned_records.csv')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f775d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37098725</td>\n",
       "      <td>36808565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37098725</td>\n",
       "      <td>30049387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37098725</td>\n",
       "      <td>25520945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37098725</td>\n",
       "      <td>36821176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37098725</td>\n",
       "      <td>22730935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37098725</td>\n",
       "      <td>19079070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37098725</td>\n",
       "      <td>35113197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>37098725</td>\n",
       "      <td>40292703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37098725</td>\n",
       "      <td>43460605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37098725</td>\n",
       "      <td>21465499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid1      qid2  label\n",
       "0  37098725  36808565      1\n",
       "1  37098725  30049387      0\n",
       "2  37098725  25520945      0\n",
       "3  37098725  36821176      0\n",
       "4  37098725  22730935      0\n",
       "5  37098725  19079070      0\n",
       "6  37098725  35113197      0\n",
       "7  37098725  40292703      0\n",
       "8  37098725  43460605      0\n",
       "9  37098725  21465499      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading ground truth table\n",
    "pd_grdtrth = pd.read_csv('data/cleaned_grnd_truth.csv')\n",
    "pd_grdtrth[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbb0290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_trth_list = pd_grdtrth[['qid1','qid2','label']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "672f831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grd_tup_list = [(each[0], each[1], each[2]) for each in grd_trth_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cdf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10c948dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexer function\n",
    "def indexing_docs(file_path,similarity,index_pth_str=\"index/\"):\n",
    "    lucene.initVM()\n",
    "    indexPath = File(index_pth_str).toPath()\n",
    "    indexDir = FSDirectory.open(indexPath)\n",
    "    writerConfig = IndexWriterConfig(EnglishAnalyzer())\n",
    "    writerConfig.setSimilarity(similarity)\n",
    "    writer = IndexWriter(indexDir, writerConfig)\n",
    "   \n",
    "    t1 = FieldType()\n",
    "    t1.setStored(True)\n",
    "    t1.setTokenized(False)\n",
    "    t1.setIndexOptions(IndexOptions.NONE)\n",
    "   \n",
    "    t2 = FieldType()\n",
    "    t2.setStored(False)\n",
    "    t2.setTokenized(True)\n",
    "    t2.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "   \n",
    "    df_file = pd.read_csv(file_path)\n",
    "    cntr = 0\n",
    "    for i, row in df_file.iterrows():\n",
    "        doc = Document()\n",
    "        doc.add(Field(\"qid\", row['qid'], t1))\n",
    "        doc.add(Field(\"qn_title\", row['qn_title'], t1))\n",
    "        doc.add(Field(\"qn_link\", row['qn_link'], t1))\n",
    "        doc.add(Field(\"qns_title_processed\", row['qns_title_processed'], t2))\n",
    "        doc.add(Field(\"qns_body_processed\", row['qns_body_processed'], t2))\n",
    "        doc.add(Field(\"ans_body_processed\", row['ans_body_processed'], t2))\n",
    "        writer.addDocument(doc)\n",
    "        cntr += 1\n",
    "    \n",
    "    print(\"Indexing Successful\")   \n",
    "    writer.close()\n",
    "    indexDir.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea69c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d308c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retreiver function\n",
    "def retriever_fn(search_string, similarity, index_pth_str=\"index/\"):\n",
    "    \n",
    "    analyzer = EnglishAnalyzer()\n",
    "    indexPath = File(index_pth_str).toPath()\n",
    "    indexDir = FSDirectory.open(indexPath)\n",
    "    reader = DirectoryReader.open(indexDir)\n",
    "    searcher = IndexSearcher(reader)\n",
    "    searcher.setSimilarity(similarity)\n",
    "\n",
    "    fields = ['qns_title_processed', 'qns_body_processed', 'ans_body_processed']\n",
    "    flags = [BooleanClause.Occur.SHOULD, BooleanClause.Occur.SHOULD, BooleanClause.Occur.SHOULD]\n",
    "    query_parser = MultiFieldQueryParser(fields, analyzer)\n",
    "    query = query_parser.parse(search_string, fields, flags, analyzer)\n",
    "    MAX = 50\n",
    "    start_time = time.time()\n",
    "    hits = searcher.search(query, MAX)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    indexDir.close()\n",
    "    \n",
    "    results_list = []\n",
    "    for ind,hit in enumerate(hits.scoreDocs):\n",
    "        doc = searcher.doc(hit.doc)\n",
    "        results_list.append((int(doc.get(\"qid\")), float(hit.score)))\n",
    "        \n",
    "    return results_list, retrieval_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284fab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a51a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_ranking, query_id, qrels, k=5):\n",
    "\n",
    "  \n",
    "    retrieved = [doc[0] for doc in doc_ranking[:k]] # take only the document id, rather than score\n",
    "\n",
    "    qrels_query = [qrel for qrel in qrels if qrel[0] == query_id] # iterate through the relevance judgements and return rows which are relevant to given query\n",
    "    relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 1] # retrieve the ids of documents that have positive relevance judgements (i.e relevant documents)\n",
    "    non_relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 0] # retrieve the ids of documents that have 0 relevance judgements (i.e non relevant documents)\n",
    "\n",
    "    TP = len(set(retrieved) & set(relevant_doc_ids)) # intersection between retrieved documents and relevant documents. num of docs in intersection = TP (positive examples that are correctly identified)\n",
    "    FP = len(set(retrieved) & set(non_relevant_doc_ids)) # intersection between retrieved documents and non relevant documents. num of docs in interesetion is FP (negative examples that are incorrectly identifed are positive)\n",
    "\n",
    "    if TP+FP >0:\n",
    "        precision = TP / (TP + FP)\n",
    "       \n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "\n",
    "    return TP, FP, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf87044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecc715a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_at_k(doc_ranking, query_id, qrels, k=5):\n",
    "  \n",
    "    retrieved = [doc[0] for doc in doc_ranking[:k]] # take only the document id, rather than score\n",
    "  \n",
    "    qrels_query = [qrel for qrel in qrels if qrel[0] == query_id] # iterate through the relevance judgements and return rows which are relevant to given query\n",
    "    relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 1] # retrieve the ids of documents that have positive relevance judgements (i.e relevant documents)\n",
    "    non_relevant_doc_ids = [qrel[1] for qrel in qrels_query if qrel[-1] == 0] # retrieve the ids of documents that have 0 relevance judgements (i.e non relevant documents)\n",
    "\n",
    "    TP = len(set(retrieved) & set(relevant_doc_ids)) # intersection between retrieved documents and relevant documents. num of docs in intersection = TP (positive examples that are correctly identified)\n",
    "    FP = len(set(retrieved) & set(non_relevant_doc_ids)) # intersection between retrieved documents and non relevant documents. num of docs in interesetion is FP (negative examples that are incorrectly identifed are positive)\n",
    "    FN = len(set(relevant_doc_ids) - set(retrieved)) # relevance docs minus the retrieved docs equal FN (positive examples that are incorrectly identified as negative)\n",
    "\n",
    "    if TP+FP >0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "        \n",
    "    if TP+FN >0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "        \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)  \n",
    "    else:\n",
    "        f1 = 0\n",
    "  \n",
    "    return f1, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff90dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43030e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = pd.read_csv('data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ebcda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = pd_test[['qid','query']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545092a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cafc8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 1\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = ClassicSimilarity()\n",
    "index_pth = \"index1/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9aa8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.5570175438596492\n",
      "Average Recall at 5 = 0.6052631578947368\n",
      "Average F-1 Score at 5 = 0.56265664160401\n",
      "Average Retrieval Time =  0.0007008376874421772\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0eee12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5228070175438596\n",
      "Average Recall at 10 = 0.6052631578947368\n",
      "Average F-1 Score at 10 = 0.5394736842105263\n",
      "Average Retrieval Time =  0.001544927295885588\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf5330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b1c408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 2\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BooleanSimilarity()\n",
    "index_pth = \"index2/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3125f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.5263157894736842\n",
      "Average Recall at 5 = 0.5\n",
      "Average F-1 Score at 5 = 0.5087719298245613\n",
      "Average Retrieval Time =  0.0005468569303813734\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0963f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5263157894736842\n",
      "Average Recall at 10 = 0.5\n",
      "Average F-1 Score at 10 = 0.5087719298245613\n",
      "Average Retrieval Time =  0.0009108593589381167\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2616899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d21ab493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 3\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = LMDirichletSimilarity()\n",
    "index_pth = \"index3/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8eef3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.3815789473684211\n",
      "Average Recall at 5 = 0.42105263157894735\n",
      "Average F-1 Score at 5 = 0.39598997493734334\n",
      "Average Retrieval Time =  0.0017100760811253597\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f57ce394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.4789473684210526\n",
      "Average Recall at 10 = 0.5\n",
      "Average F-1 Score at 10 = 0.4780701754385964\n",
      "Average Retrieval Time =  0.0012603433508622018\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172e1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54858177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 4\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BM25Similarity()\n",
    "index_pth = \"index4/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4bde1696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.5394736842105263\n",
      "Average Recall at 5 = 0.5526315789473685\n",
      "Average F-1 Score at 5 = 0.5363408521303258\n",
      "Average Retrieval Time =  0.0009391809764661287\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6d12681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5263157894736842\n",
      "Average Recall at 10 = 0.6052631578947368\n",
      "Average F-1 Score at 10 = 0.543859649122807\n",
      "Average Retrieval Time =  0.0013090309343839947\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c143a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4976c95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 5\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BM25Similarity(1.0,0.5)\n",
    "index_pth = \"index5/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59a8777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.5394736842105263\n",
      "Average Recall at 5 = 0.5526315789473685\n",
      "Average F-1 Score at 5 = 0.5363408521303258\n",
      "Average Retrieval Time =  0.0007241901598478618\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c712471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5263157894736842\n",
      "Average Recall at 10 = 0.6052631578947368\n",
      "Average F-1 Score at 10 = 0.543859649122807\n",
      "Average Retrieval Time =  0.0007389595634058902\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615735a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d322b990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing Successful\n"
     ]
    }
   ],
   "source": [
    "#Test - 6\n",
    "\n",
    "lucene.initVM()\n",
    "sim_measure = BM25Similarity(6.0,0.5)\n",
    "index_pth = \"index6/\"\n",
    "indexing_docs(\"data/cleaned_records.csv\",sim_measure,index_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "757dd148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 5 = 0.4342105263157895\n",
      "Average Recall at 5 = 0.4473684210526316\n",
      "Average F-1 Score at 5 = 0.43107769423558895\n",
      "Average Retrieval Time =  0.001201177898206209\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd08fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at 10 = 0.5175438596491228\n",
      "Average Recall at 10 = 0.6052631578947368\n",
      "Average F-1 Score at 10 = 0.5350877192982457\n",
      "Average Retrieval Time =  0.0011200277428877982\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "pr_list =[]\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "rt_time = []\n",
    "\n",
    "for ind, each in enumerate(test_list):\n",
    "    new_str = each[1].translate(table)\n",
    "    retreived_docs, retrieval_time = retriever_fn(new_str, sim_measure, index_pth)\n",
    "    rt_time.append(retrieval_time)\n",
    "    tp, fp, precision = precision_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    pr_list.append(precision)\n",
    "    f1_score, recall = f1_score_at_k(retreived_docs, each[0], grd_tup_list, k=k)\n",
    "    f1_list.append(f1_score)\n",
    "    rec_list.append(recall)\n",
    "    \n",
    "    \n",
    "print('Average Precision at {} = {}'.format(k, sum(pr_list)/float(len(pr_list))))\n",
    "print('Average Recall at {} = {}'.format(k, sum(rec_list)/float(len(rec_list))))\n",
    "print('Average F-1 Score at {} = {}'.format(k, sum(f1_list)/float(len(f1_list))))\n",
    "print('Average Retrieval Time = ', sum(rt_time)/float(len(rt_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94122e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d7f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab39ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c4931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
